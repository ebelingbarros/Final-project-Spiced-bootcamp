{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with oil and renewable energies project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import wordcloud\n",
    "import requests, json\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "import re\n",
    "import itertools\n",
    "import unicodedata\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_oil_price=pd.read_csv('data/corpus_oil_price_3.csv', index_col=0)\n",
    "corpus_alternative_energies=pd.read_csv('data/corpus_alternative_energies_3.csv', index_col=0)\n",
    "corpus_crude_oil=pd.read_csv('data/corpus_crude_oil_3.csv', index_col=0)\n",
    "corpus_oil_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_oil_price[\"date_published\"] = pd.to_datetime(corpus_oil_price[\"date_published\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_oil_price['year'] = corpus_oil_price['date_published'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = corpus_oil_price.loc[corpus_oil_price['year']==2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"[\",\"\")\n",
    "filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"]\",\"\")\n",
    "filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"'\",\"\")\n",
    "filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"',\",\"\")\n",
    "filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\",'\",\"\")\n",
    "filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_one_list_2 = filtered_df['most_common_words'].tolist()\n",
    "col_one_list = []\n",
    "for i in col_one_list_2:\n",
    "    listRes = list(i.split(\" \"))\n",
    "    col_one_list.append(listRes)   \n",
    "col_one_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\",\" \"]\n",
    "for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "col_one_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt = {}\n",
    "for words in col_one_list:\n",
    "    for word in words:\n",
    "        if word not in word_cnt:\n",
    "            word_cnt[word] = 1\n",
    "        else:\n",
    "            word_cnt[word] += 1\n",
    "    \n",
    "word_cnt_df = pd.DataFrame({'word': [k for k in word_cnt.keys()], 'cnt': [v for v in word_cnt.values()]})\n",
    "len(word_cnt_df)  \n",
    "word_cnt_df.sort_values(by=['cnt'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "target_words = word_cnt_df[word_cnt_df['cnt'] > 3]['word'].to_numpy()\n",
    "for word in target_words:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "re_vocab = {}\n",
    "for word, i in vocab.items():\n",
    "    re_vocab[i] = word\n",
    "    \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [list(itertools.combinations(words, 2)) for words in col_one_list]\n",
    "combination_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "for tweet_comb in combinations:\n",
    "    for comb in tweet_comb:\n",
    "        if comb[0] in target_words and comb[1] in target_words:\n",
    "            combination_matrix[vocab[comb[0]], vocab[comb[1]]] += 1\n",
    "            combination_matrix[vocab[comb[1]], vocab[comb[0]]] += 1\n",
    "        \n",
    "for i in range(len(vocab)):\n",
    "    combination_matrix[i, i] /= 2\n",
    "        \n",
    "combination_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_matrix = 1 - distance.cdist(combination_matrix, combination_matrix, 'jaccard')\n",
    "jaccard_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    for j in range(i+1, len(vocab)):\n",
    "        jaccard = jaccard_matrix[i, j]\n",
    "        if jaccard > 0:\n",
    "            nodes.append([re_vocab[i], re_vocab[j], word_cnt[re_vocab[i]], word_cnt[re_vocab[j]], jaccard])\n",
    "            \n",
    "\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.nodes(data=True)\n",
    "\n",
    "for pair in nodes:\n",
    "    node_x, node_y, node_x_cnt, node_y_cnt, jaccard = pair[0], pair[1], pair[2], pair[3], pair[4]\n",
    "    if not G.has_node(node_x):\n",
    "        G.add_node(node_x, count=node_x_cnt)\n",
    "    if not G.has_node(node_y):\n",
    "        G.add_node(node_y, count=node_y_cnt)\n",
    "    if not G.has_edge(node_x, node_y):\n",
    "        G.add_edge(node_x, node_y, weight=jaccard)\n",
    "        \n",
    "plt.figure(figsize=(15,15))\n",
    "pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "node_size = [d['count']*150 for (n,d) in G.nodes(data=True)]\n",
    "nx.draw_networkx_nodes(G, pos, node_color='cyan', alpha=1.0, node_size=node_size)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "edge_width = [d['weight']*10 for (u,v,d) in G.edges(data=True)]\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.1, edge_color='grey', width=edge_width)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ano in range(2010, 2022):\n",
    "    corpus_oil_price[\"date_published\"] = pd.to_datetime(corpus_oil_price[\"date_published\"], utc=True)\n",
    "    corpus_oil_price['year'] = corpus_oil_price['date_published'].dt.year\n",
    "    filtered_df = corpus_oil_price.loc[corpus_oil_price['year']==ano]\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"[\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"]\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"'\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"',\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\",'\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\",\",\"\")\n",
    "    col_one_list_2 = filtered_df['most_common_words'].tolist()\n",
    "    col_one_list = []\n",
    "    for i in col_one_list_2:\n",
    "        listRes = list(i.split(\" \"))\n",
    "        col_one_list.append(listRes)   \n",
    "\n",
    "    stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\", \" \"]\n",
    "    for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "    \n",
    "    stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\", \" \"]\n",
    "    for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "    \n",
    "    stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\", \" \"]\n",
    "    for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "    \n",
    "    word_cnt = {}\n",
    "    for words in col_one_list:\n",
    "        for w in words:\n",
    "            if w not in word_cnt:\n",
    "                word_cnt[w] = 1\n",
    "            else:\n",
    "                word_cnt[w] += 1\n",
    "    \n",
    "    word_cnt_df = pd.DataFrame({'word': [k for k in word_cnt.keys()], 'cnt': [v for v in word_cnt.values()]})\n",
    "    word_cnt_df.sort_values(by=['cnt'])\n",
    "    vocab = {}\n",
    "    target_words = word_cnt_df[word_cnt_df['cnt'] > 3]['word'].to_numpy()\n",
    "    for word in target_words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    re_vocab = {}\n",
    "    for word, i in vocab.items():\n",
    "        re_vocab[i] = word        \n",
    "    \n",
    "    combinations = [list(itertools.combinations(words, 2)) for words in col_one_list]\n",
    "    combination_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for tweet_comb in combinations:\n",
    "        for comb in tweet_comb:\n",
    "            if comb[0] in target_words and comb[1] in target_words:\n",
    "                combination_matrix[vocab[comb[0]], vocab[comb[1]]] += 1\n",
    "                combination_matrix[vocab[comb[1]], vocab[comb[0]]] += 1\n",
    "        \n",
    "    for i in range(len(vocab)):\n",
    "            combination_matrix[i, i] /= 2\n",
    "        \n",
    "    jaccard_matrix = 1 - distance.cdist(combination_matrix, combination_matrix, 'jaccard')\n",
    "    nodes = []\n",
    "    for i in range(len(vocab)):\n",
    "        for j in range(i+1, len(vocab)):\n",
    "            jaccard = jaccard_matrix[i, j]\n",
    "            if jaccard > 0:\n",
    "                nodes.append([re_vocab[i], re_vocab[j], word_cnt[re_vocab[i]], word_cnt[re_vocab[j]], jaccard])\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.nodes(data=True)\n",
    "    for pair in nodes:\n",
    "        node_x, node_y, node_x_cnt, node_y_cnt, jaccard = pair[0], pair[1], pair[2], pair[3], pair[4]\n",
    "        if not G.has_node(node_x):\n",
    "            G.add_node(node_x, count=node_x_cnt)\n",
    "        if not G.has_node(node_y):\n",
    "            G.add_node(node_y, count=node_y_cnt)\n",
    "        if not G.has_edge(node_x, node_y):\n",
    "            G.add_edge(node_x, node_y, weight=jaccard)\n",
    "        \n",
    "    plt.figure(figsize=(20,15))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    node_size = [d['count']*150 for (n,d) in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='cyan', alpha=1.0, node_size=node_size)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "    edge_width = [d['weight']*7 for (u,v,d) in G.edges(data=True)]\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='grey', width=edge_width)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig('images/oil'+str(ano)+'.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ano in range(2010, 2022):\n",
    "    corpus_alternative_energies[\"date_published\"] = pd.to_datetime(corpus_alternative_energies[\"date_published\"], utc=True)\n",
    "    corpus_alternative_energies['year'] = corpus_alternative_energies['date_published'].dt.year\n",
    "    filtered_df = corpus_alternative_energies.loc[corpus_alternative_energies['year']==ano]\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"[\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"]\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"'\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\"',\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\",'\",\"\")\n",
    "    filtered_df['most_common_words'] = filtered_df['most_common_words'].str.replace(\",\",\"\")\n",
    "    col_one_list_2 = filtered_df['most_common_words'].tolist()\n",
    "    col_one_list = []\n",
    "    for i in col_one_list_2:\n",
    "        listRes = list(i.split(\" \"))\n",
    "        col_one_list.append(listRes)   \n",
    "\n",
    "    stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\", \" \", \"energy\", \"power\"]\n",
    "    for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "    \n",
    "    stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\", \" \", \"energy\", \"power\"]\n",
    "    for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "    \n",
    "    stopwords = [\"$\",\"d\",\"oil\",\"prices\",\"price\",\"\", \" \", \"energy\", \"power\"]\n",
    "    for c in col_one_list:\n",
    "        #for i in range(len(c)):\n",
    "        #    c[i] = c[i].lower()\n",
    "        for word in c:  # iterating on a copy since removing will mess things up\n",
    "            if word in stopwords:\n",
    "                c.remove(word)\n",
    "    \n",
    "    word_cnt = {}\n",
    "    for words in col_one_list:\n",
    "        for w in words:\n",
    "            if w not in word_cnt:\n",
    "                word_cnt[w] = 1\n",
    "            else:\n",
    "                word_cnt[w] += 1\n",
    "    \n",
    "    word_cnt_df = pd.DataFrame({'word': [k for k in word_cnt.keys()], 'cnt': [v for v in word_cnt.values()]})\n",
    "    word_cnt_df.sort_values(by=['cnt'])\n",
    "    vocab = {}\n",
    "    target_words = word_cnt_df[word_cnt_df['cnt'] > 3]['word'].to_numpy()\n",
    "    for word in target_words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    re_vocab = {}\n",
    "    for word, i in vocab.items():\n",
    "        re_vocab[i] = word        \n",
    "    \n",
    "    combinations = [list(itertools.combinations(words, 2)) for words in col_one_list]\n",
    "    combination_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for tweet_comb in combinations:\n",
    "        for comb in tweet_comb:\n",
    "            if comb[0] in target_words and comb[1] in target_words:\n",
    "                combination_matrix[vocab[comb[0]], vocab[comb[1]]] += 1\n",
    "                combination_matrix[vocab[comb[1]], vocab[comb[0]]] += 1\n",
    "        \n",
    "    for i in range(len(vocab)):\n",
    "            combination_matrix[i, i] /= 2\n",
    "        \n",
    "    jaccard_matrix = 1 - distance.cdist(combination_matrix, combination_matrix, 'jaccard')\n",
    "    nodes = []\n",
    "    for i in range(len(vocab)):\n",
    "        for j in range(i+1, len(vocab)):\n",
    "            jaccard = jaccard_matrix[i, j]\n",
    "            if jaccard > 0:\n",
    "                nodes.append([re_vocab[i], re_vocab[j], word_cnt[re_vocab[i]], word_cnt[re_vocab[j]], jaccard])\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.nodes(data=True)\n",
    "    for pair in nodes:\n",
    "        node_x, node_y, node_x_cnt, node_y_cnt, jaccard = pair[0], pair[1], pair[2], pair[3], pair[4]\n",
    "        if not G.has_node(node_x):\n",
    "            G.add_node(node_x, count=node_x_cnt)\n",
    "        if not G.has_node(node_y):\n",
    "            G.add_node(node_y, count=node_y_cnt)\n",
    "        if not G.has_edge(node_x, node_y):\n",
    "            G.add_edge(node_x, node_y, weight=jaccard)\n",
    "        \n",
    "    plt.figure(figsize=(20,15))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    node_size = [d['count']*150 for (n,d) in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightgreen', alpha=1.0, node_size=node_size)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "    edge_width = [d['weight']*7 for (u,v,d) in G.edges(data=True)]\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, edge_color='grey', width=edge_width)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig('images/energy'+str(ano)+'.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
